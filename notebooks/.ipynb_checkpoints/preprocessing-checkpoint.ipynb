{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d093ffb-5112-4da8-aa71-aa9d60d71d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prady\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import tqdm\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as spacy_stop_words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as nltk_stop_words\n",
    "import spacy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='seaborn')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')\n",
    "\n",
    "\n",
    "# defining the lemmatizer and stopwords\n",
    "STOP_WORDS = set(spacy_stop_words).union(set(nltk_stop_words.words(\"english\")))\n",
    "SPACY_TOKENIZER = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abcd945f-6a05-487a-bbfc-bf0a923a8035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data\n",
    "df = pd.read_csv(\"../data/raw.csv\")\n",
    "\n",
    "# removing the unwanted column\n",
    "df.drop(columns=['article_id.1'],inplace=True)\n",
    "\n",
    "# making the date as datetime dtype\n",
    "df['published date'] = pd.to_datetime(df['published date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6a45cdb-4d78-424a-8f59-200a920dc5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>published date</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>asia media</td>\n",
       "      <td>asia - business &amp; finance</td>\n",
       "      <td>EY achieves highest growth in nearly two decad...</td>\n",
       "      <td>2022-09-21 07:00:00</td>\n",
       "      <td>US$3.2b invested in audit quality, innovation,...</td>\n",
       "      <td>{'href': 'https://www.ey.com', 'title': 'Ernst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>asia media</td>\n",
       "      <td>asia - business &amp; finance</td>\n",
       "      <td>Illuminate Financial Announces Strategic Partn...</td>\n",
       "      <td>2022-11-29 08:00:00</td>\n",
       "      <td>LONDON, Nov. 29, 2022 /PRNewswire/ -- Illumina...</td>\n",
       "      <td>{'href': 'https://finance.yahoo.com', 'title':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>asia media</td>\n",
       "      <td>asia - business &amp; finance</td>\n",
       "      <td>Philip Morris International Announces New Regi...</td>\n",
       "      <td>2022-11-25 08:00:00</td>\n",
       "      <td>LAUSANNE, Switzerland, November 25, 2022--(BUS...</td>\n",
       "      <td>{'href': 'https://finance.yahoo.com', 'title':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>asia media</td>\n",
       "      <td>asia - business &amp; finance</td>\n",
       "      <td>18 Thailand Companies and Entrepreneurs Win Co...</td>\n",
       "      <td>2022-07-02 07:00:00</td>\n",
       "      <td>SINGAPORE, July 2, 2022 /PRNewswire/ -- Mr. Wi...</td>\n",
       "      <td>{'href': 'https://finance.yahoo.com', 'title':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>asia media</td>\n",
       "      <td>asia - business &amp; finance</td>\n",
       "      <td>Meihua International Medical Technologies Co.,...</td>\n",
       "      <td>2022-12-05 08:00:00</td>\n",
       "      <td>YANGZHOU, China, Dec. 5, 2022 /PRNewswire/ -- ...</td>\n",
       "      <td>{'href': 'https://finance.yahoo.com', 'title':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id    category                subcategory  \\\n",
       "0           0  asia media  asia - business & finance   \n",
       "1           1  asia media  asia - business & finance   \n",
       "2           2  asia media  asia - business & finance   \n",
       "3           3  asia media  asia - business & finance   \n",
       "4           4  asia media  asia - business & finance   \n",
       "\n",
       "                                               title      published date  \\\n",
       "0  EY achieves highest growth in nearly two decad... 2022-09-21 07:00:00   \n",
       "1  Illuminate Financial Announces Strategic Partn... 2022-11-29 08:00:00   \n",
       "2  Philip Morris International Announces New Regi... 2022-11-25 08:00:00   \n",
       "3  18 Thailand Companies and Entrepreneurs Win Co... 2022-07-02 07:00:00   \n",
       "4  Meihua International Medical Technologies Co.,... 2022-12-05 08:00:00   \n",
       "\n",
       "                                                text  \\\n",
       "0  US$3.2b invested in audit quality, innovation,...   \n",
       "1  LONDON, Nov. 29, 2022 /PRNewswire/ -- Illumina...   \n",
       "2  LAUSANNE, Switzerland, November 25, 2022--(BUS...   \n",
       "3  SINGAPORE, July 2, 2022 /PRNewswire/ -- Mr. Wi...   \n",
       "4  YANGZHOU, China, Dec. 5, 2022 /PRNewswire/ -- ...   \n",
       "\n",
       "                                              source  \n",
       "0  {'href': 'https://www.ey.com', 'title': 'Ernst...  \n",
       "1  {'href': 'https://finance.yahoo.com', 'title':...  \n",
       "2  {'href': 'https://finance.yahoo.com', 'title':...  \n",
       "3  {'href': 'https://finance.yahoo.com', 'title':...  \n",
       "4  {'href': 'https://finance.yahoo.com', 'title':...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aaee52-cb71-4211-b22b-b2278f40e544",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd591d1-ccc2-4e27-8b68-042fca6b90ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function which splits given article into paragraphs\n",
    "def text_to_paragraphs(text:str)->list:\n",
    "    \"\"\"\n",
    "    splits given article into paragraphs and returns list of paragraphs\n",
    "    \n",
    "    \"\"\"\n",
    "    # if the text is not a str then return an empty list\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # split the text into paragraphs\n",
    "    paragraphs = list(filter(lambda x: x!=\"\", text.split(\"\\n\")))\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a4c9c9a-b941-44f2-acf4-33a4342ac8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sentences(text:str)->list:\n",
    "    \"\"\"\n",
    "    splits the given text into sentences and returns a list of sentences\n",
    "    \"\"\"\n",
    "    # if the text is not a str then return an empty list\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "     # split the text into sentences\n",
    "    sentences = SPACY_TOKENIZER(text).sents\n",
    "    sentences = [str(sentence) for sentence in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eefcade-0c94-43f6-a070-b1b13d3a6d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to seprarate capitalized words\n",
    "def separate_capitalilzed_words(text:str)->str:\n",
    "    \"\"\"\n",
    "    \"ThisIsAWord\" -> \"This Is A Word\"\n",
    "    \n",
    "    \"\"\"\n",
    "    assert isinstance(text, str)\n",
    "    text = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f376a-e445-45dd-965b-a62717648730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecd4b4ef-c7bd-42df-81e4-e6c88085e5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\prady\\AppData\\Local\\Temp\\ipykernel_21172\\1944091441.py:15: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  pattern = '[^a-zA-Z0-9%\\ \\n]+'\n"
     ]
    }
   ],
   "source": [
    "# defining a function to preprocess the text\n",
    "def preprocess_text(text:str)->str:\n",
    "    \"\"\"\n",
    "    text : string to preprocess\n",
    "    returns preprocessed text\n",
    "    \"\"\"\n",
    "    \n",
    "    # typecasting text to the str\n",
    "    text = str(text)\n",
    "    # separating the capitalized words\n",
    "    text = separate_capitalilzed_words(text)\n",
    "    # lowercasing all letters in the text\n",
    "    text = text.lower()\n",
    "    # removing non_alphanumeric elements\n",
    "    pattern = '[^a-zA-Z0-9%\\ \\n]+'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    # removing digits \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # fixing white spaces\n",
    "    text = \"\\n\".join(\" \".join(text.split()).split(\"\\n\"))\n",
    "    # removing stopwords\n",
    "    text = \" \".join([word for word in text.lower().split() if word not in STOP_WORDS])\n",
    "    # tokenization\n",
    "    text = \" \".join([token.text for token in SPACY_TOKENIZER(text)])\n",
    "    # lemmatization\n",
    "    text = \" \".join([token.lemma_ for token in SPACY_TOKENIZER(text)])\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad635baa-3df4-4cfe-b751-68237eb79396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to pre-process (splitting the article into sentences or paragraphs each split is called a section)\n",
    "def preprocess_article(article_id:int,title:str,text:str,section_by:str=\"paragraph\",input_type=['title','text'])->list:\n",
    "    \"\"\"\n",
    "    article_id    : unique ID of the article\n",
    "    title         : title of the article\n",
    "    text          : text/body of the article\n",
    "    section_by    : based on what we are going to split our article (eg: sentence,paragraph) default is by paragraph\n",
    "    input_type   : which features are we going to consider eg: ['title'],['text'],['title','text'] default is ['title','text']\n",
    "    \n",
    "    \"\"\"\n",
    "    # if we are going to use only 'title' for generating embeddings\n",
    "    if input_type==['title']:    \n",
    "        sections = [preprocess_text(title)]\n",
    "        \n",
    "    # if we are going to use 'text' feature for generating embeddings\n",
    "    elif input_type==['text']:    \n",
    "        if section_by==\"paragraph\": # if we are going to split the text into paragraphs\n",
    "            sections = text_to_paragraphs(text)\n",
    "        elif section_by==\"sentence\":  # if we are going to split the text into sentences\n",
    "            sections = text_to_sentences(text)\n",
    "        elif section_by==None: # if we are not going to split\n",
    "            sections = [text]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid section_by value, it must be either 'paragraph' or 'sentence'.\")\n",
    "            \n",
    "    # if we are going to use both 'title' and 'text' features\n",
    "    elif input_type==['title','text']:\n",
    "        if section_by==\"paragraph\":  # if we are going to split the text into paragraphs\n",
    "            sections = text_to_paragraphs(text)\n",
    "        elif section_by==\"sentence\":  # if we are going to split the text into sentences\n",
    "            sections = text_to_sentences(text)\n",
    "        elif section_by==None:\n",
    "            sections = [text]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid section_by value, it must be either 'paragraph' or 'sentence'.\")\n",
    "        # preprocessing the 'title'\n",
    "        title = preprocess_text(title)\n",
    "        # prprocessing the sections (text splitted into)\n",
    "        section = list(map(preprocess_text,sections))\n",
    "        # combining the 'title' and sections \n",
    "        sections = [title] + section\n",
    "        # filtering the sentences which are greater than 2 words\n",
    "        sections = list(filter(lambda x:len(x.split())>2,sections))\n",
    "\n",
    "    # edge case\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input_type value. Allowed values are ['title'], ['text'] and ['title', 'text']\")\n",
    "\n",
    "    # getting the no of splits \n",
    "    section_count = len(sections)\n",
    "\n",
    "    return list(zip([article_id]*section_count,sections))  # [ (1,sections), (2,sections) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68f40938-075a-43de-80da-b0d4d51e2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function which preprocess the data\n",
    "def preprocess_data(raw_file_path,section_by,input_type):\n",
    "    \n",
    "    \"\"\"\n",
    "    raw_file_path   : path of the raw data\n",
    "    section_by      : based on what we are going to split our article (eg: sentence,paragraph)\n",
    "    input_type      : which features are we going to consider eg: ['title'],['text'],['title','text']\n",
    "    \n",
    "    \"\"\"\n",
    "    # reading the data from the raw_file_path\n",
    "    data = pd.read_csv(raw_file_path,nrows=5)\n",
    "\n",
    "    # if we don't want to split our article\n",
    "    if section_by==None:\n",
    "        \n",
    "        if input_type==['title']:   # if we are going to use only title feature for embedding\n",
    "            data = data[['article_id','title']]\n",
    "            data['title'] = data['title'].apply(preprocess_text)  \n",
    "        elif input_type==['text']:  # if we are going to use only text feature for embedding\n",
    "            data = data[['article_id','text']]\n",
    "            data['text'] = data['text'].apply(preprocess_text)\n",
    "        else:  # if we want to use both title and text for embeddings\n",
    "            a = list(reduce(lambda x, y: x+y, map(lambda x: [(x[0], preprocess_text(x[1])), (x[0], preprocess_text(x[2]))], tqdm.tqdm(data[[\"article_id\", \"title\", \"text\"]].values, desc=\"Processing data\", leave=True))))\n",
    "            data = pd.DataFrame(a,columns=[\"article_id\", \"text\"])\n",
    "\n",
    "\n",
    "    # if we want to split the text by either parapgraph or sentence wise\n",
    "    elif section_by==\"paragraph\" or section_by==\"sentence\":\n",
    "            data = list(reduce(lambda x, y: x+y, map(lambda x: preprocess_article(x[0],x[1],x[2],section_by,input_type), tqdm.tqdm(data[[\"article_id\", \"title\", \"text\"]].values, desc=\"Processing data\", leave=True))))\n",
    "            data = pd.DataFrame(data,columns=['article_id','text'])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid section_by value. Allowed values are 'paragraph' and 'sentence' and None\")\n",
    "    \n",
    "    # generating id for each splits\n",
    "    data['section_id']= range(data.shape[0])\n",
    "\n",
    "    # generating a mapping from article_id to section_id to find the article_id for a given section_id by creating  a dictionary where article_id is the key and corresponding section_ids is the value\n",
    "    article_section_mapping = data.groupby('article_id')['section_id'].apply(list).to_dict()\n",
    "\n",
    "    # generating a mapping from section_id to article_id to find the article_id of a given section_id by creating  a dictionary where section_id is the key and the corresponding article_id is the value\n",
    "    section_article_mapping = data.set_index('section_id')['article_id'].to_dict()\n",
    "\n",
    "    # Pickling the dictionaries\n",
    "    article_section_mapping_path = '../artifacts/article_section_mapping.pkl'\n",
    "    with open(article_section_mapping_path, 'wb') as f:\n",
    "        pickle.dump(article_section_mapping, f)\n",
    "\n",
    "    section_article_mapping_path = '../artifacts/section_article_mapping.pkl'\n",
    "    with open(section_article_mapping_path, 'wb') as f:\n",
    "        pickle.dump(section_article_mapping, f)\n",
    "\n",
    "    return data,article_section_mapping_path,section_article_mapping_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378075dc-812c-47a8-9c2a-be7c1d702d2c",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6b72bd1-7b15-440a-98c7-ff62742b590c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 5/5 [00:04<00:00,  1.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>text</th>\n",
       "      <th>section_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ey achieve high growth nearly decade report re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>usb invest audit quality innovation technology...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>mark successful year history organization high...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>ey today publish value realize report expand a...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>carmine di sibio ey global chairman ceo say ey...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                               text  section_id\n",
       "0           0  ey achieve high growth nearly decade report re...           0\n",
       "1           0  usb invest audit quality innovation technology...           1\n",
       "2           0  mark successful year history organization high...           2\n",
       "3           0  ey today publish value realize report expand a...           3\n",
       "4           0  carmine di sibio ey global chairman ceo say ey...           4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the function\n",
    "preprocessed_df1,article_section_mapping_path,section_article_mapping_path = preprocess_data(\"../data/raw.csv\",\"sentence\",['title','text'])\n",
    "preprocessed_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f5c22b-fe8f-44f3-83e0-c79ff4e68b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# BUCKET_NAME=\"search-relevancy\"\n",
    "# FILE_NAME=\"preprocessed.csv\"\n",
    "# pradyu=pd.read_csv(f's3://{BUCKET_NAME}/{FILE_NAME}',index_col=0)\n",
    "# pradyu.article_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c5a1c7-aca2-49e9-9228-cb87427a4b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53676ad5-57ab-417e-8f08-6e649e4e9197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c95b37f-0d47-4d22-870a-689950d91160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
